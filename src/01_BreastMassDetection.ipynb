{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Breast mass detection**\n",
    "##### *Advanced Image Analysis 2022-2023*\n",
    "##### *Luisana Álvarez Monsalve, Clara Lisazo, Xavier Beltrán Urbano*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Import libraries and set paths of images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from cv2 import ximgproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_images = \"/Users/clara/Desktop/MAIA/AIA/dataset/images/\"\n",
    "path_gt = \"/Users/clara/Desktop/MAIA/AIA/dataset/groundtruth/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_crop(downsampled_image, is_positive, img_gt=None):\n",
    "    '''\n",
    "    If image is negative crops the image to only contain the breast area.\n",
    "    If image is positive crops both the image and the groundtruth.\n",
    "    Args:\n",
    "        downsampled_image (numpy.ndarray): is the downsampled breast image (either positive or negative)\n",
    "        is_positive (bool): variable that determines if image is positive or negative\n",
    "        img_gt (numpy.ndarray): groundtruth image. If the image is negative, no img_gt should be passed as argument, default is None.\n",
    "    Outputs:\n",
    "        For negative images:\n",
    "            cropped_img (numpy.ndarray): cropped image\n",
    "            None: because there is no groundtruth image to crop\n",
    "        For positive images: \n",
    "            cropped_img (numpy.ndarray): cropped image\n",
    "            cropped_gt (numpy.ndarray): cropped gt\n",
    "    '''\n",
    "\n",
    "\n",
    "    _ , imgBin = cv2.threshold(downsampled_image, 0 ,255, cv2.THRESH_BINARY)\n",
    "    cnts, _ = cv2.findContours(imgBin.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = max(cnts, key = cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    cropped_img=downsampled_image[y:y+h, x:x+w]\n",
    "\n",
    "    if is_positive:\n",
    "       # Groundtruth should be cropped in the original scale, thus multiply the coordinates of bounding rectangle by the downsampling factor\n",
    "       x_originalScale = x*4\n",
    "       y_originalScale = y*4\n",
    "       w_originalScale = w*4\n",
    "       h_originalScale = h*4\n",
    "       cropped_gt=img_gt[y_originalScale:y_originalScale+h_originalScale, x_originalScale:x_originalScale+w_originalScale]\n",
    "       return cropped_img, cropped_gt\n",
    "    else: \n",
    "       return cropped_img, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clahe(image, tile_size, clip_limit):\n",
    "    '''\n",
    "    Computes the Contrast Limited Adaptive Histogram Equalization to an image.\n",
    "\n",
    "    Args:\n",
    "      image (numpy.ndarray): image to be processed\n",
    "      tile_size (int)\n",
    "      clip_limit (float)\n",
    "    Outputs:\n",
    "      CLAHE_image (numpy.ndarray): contrast enhanced image\n",
    "    '''\n",
    "\n",
    "    CLAHE = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(tile_size,tile_size))\n",
    "    CLAHE_image = CLAHE.apply(image)\n",
    "    return CLAHE_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MeanShift(image, sp, sr):\n",
    "    '''\n",
    "    Applies mean shift to input image.\n",
    "\n",
    "    Args:\n",
    "      image (numpy.ndarray): image to be processed\n",
    "      sp (int): spatial window radius\n",
    "      sr (int): color window radius \n",
    "    Output:\n",
    "      meanShiftImage (numpy.ndarray): image with the applied meanshift\n",
    "    '''\n",
    "    # Convert to three-channel image in order to satisfy requirements of OpenCV's implementation of meanshift\n",
    "    threeChannelImage = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "    meanShiftImage = cv2.pyrMeanShiftFiltering(threeChannelImage, sp, sr, None, 1)\n",
    "    meanShiftImage = cv2.cvtColor(meanShiftImage, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    return meanShiftImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiScaleMorphology(image, num_scales, size_opening, size_topHat):\n",
    "\n",
    "    ''' \n",
    "    Applies grayscale morphology at different sizes of structuring elements\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): preprocessed image\n",
    "        num_scales (int): number of scales (sizes) at which the grayscale morphology will be applied\n",
    "        size_opening (list): 1D vector of size equal to num_scales that specifies the radius of the disk-shaped \n",
    "            structuring element used for morphological opening\n",
    "        size_topHat (list): 1D vector of size equal to num_scales that specifies the radius of the disk-shaped \n",
    "            structuring element used for morphological Top Hat\n",
    "    Output:\n",
    "        morphologyList (list): a list (size = num_scales) containing the resulting images after applying the grayscale morphology operations\n",
    "\n",
    "    '''\n",
    "\n",
    "    morphology_vector_topHat = [None] * num_scales\n",
    "    morphology_vector_opening = [None] * num_scales\n",
    "    morphologyList = [None] * num_scales\n",
    "\n",
    "\n",
    "    for i in range(num_scales):\n",
    "\n",
    "        # Apply Top Hat with structuring element ellipse of size size_topHat[i]\n",
    "        structElemTopHat = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (size_topHat[i], size_topHat[i]))\n",
    "        morphology_vector_topHat[i] = cv2.morphologyEx(image, cv2.MORPH_TOPHAT, structElemTopHat)\n",
    "        \n",
    "        # Apply opening to the result of Top Hat with structuring element ellipse of size size_opening[i]\n",
    "        structElemOpening = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (size_opening[i], size_opening[i]))\n",
    "        morphology_vector_opening[i] = cv2.morphologyEx(morphology_vector_topHat[i], cv2.MORPH_OPEN, structElemOpening)\n",
    "        \n",
    "        # Linearly scale pixel values to 8-bit grayscale\n",
    "        morphologyList[i] = np.zeros_like(morphology_vector_opening[i], dtype=np.uint8)\n",
    "        cv2.normalize(morphology_vector_opening[i], morphologyList[i], 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "\n",
    "    return morphologyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussianBlurring(list_imgs, kernel_size):\n",
    "    ''' \n",
    "    Applies Gaussian blurring to all the images in list_imgs\n",
    "\n",
    "    Args:\n",
    "      list_imgs (list): list of images to which Gaussian blurring will be applied\n",
    "      kernel_size (int): size of the Gaussian kernel\n",
    "\n",
    "    Output:\n",
    "      list_imgs_blurred (list): list of Gaussian blurred images\n",
    "    '''\n",
    "    list_imgs_blurred = []\n",
    "    for i in range(len(list_imgs)):\n",
    "      img_blurred = cv2.GaussianBlur(list_imgs[i], (kernel_size, kernel_size), 0, 0)\n",
    "      list_imgs_blurred.append(img_blurred)\n",
    "\n",
    "    return list_imgs_blurred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Superpixel(list_imgs, num_scales, size_opening, num_iterations):\n",
    "    '''\n",
    "    Use Simple Linear Iterative Clustering (SLIC) for superpixel generation\n",
    "\n",
    "    Args:\n",
    "      list_imgs (list): list of images to which SLIC will be applied\n",
    "      num_scales (int): number of scales\n",
    "      size_opening (list): 1D vector of size equal to num_scales that specifies the radius of the disk-shaped \n",
    "            structuring element used for morphological opening\n",
    "      num_iterations (int): number of iteratios for the SLIC algorithm\n",
    "\n",
    "    Outputs:\n",
    "      superpixeMeanslList (list): 1D vector of size equal to num_scales that contains the images with the mean intensities inside each superpixel\n",
    "    '''\n",
    "    \n",
    "    vector_imgTesselated = [None] * num_scales\n",
    "    superpixelMeansList = [None] * num_scales\n",
    "\n",
    "    for i in range(num_scales):\n",
    "      input = list_imgs[i]\n",
    "\n",
    "      r = size_opening[i] \n",
    "      SLIC_algorithm = ximgproc.createSuperpixelSLIC(input, cv2.ximgproc.SLICO, r//4)\n",
    "      SLIC_algorithm.iterate(num_iterations)\n",
    "\n",
    "      # Get label contour mask\n",
    "      mask = SLIC_algorithm.getLabelContourMask()\n",
    "\n",
    "      # Tesselated image\n",
    "      vector_imgTesselated[i] = cv2.cvtColor(input, cv2.COLOR_GRAY2BGR)\n",
    "      vector_imgTesselated[i] = cv2.bitwise_and(vector_imgTesselated[i], vector_imgTesselated[i], mask=mask)\n",
    "      \n",
    "      labels = SLIC_algorithm.getLabels()\n",
    "\n",
    "      # Replace original image pixels with superpixels means\n",
    "      superpixelMeansList[i] = input.copy()\n",
    "\n",
    "      for k in range(SLIC_algorithm.getNumberOfSuperpixels()):\n",
    "          # Create a binary mask for the current superpixel label\n",
    "          class_mask = cv2.inRange(labels, k, k)\n",
    "          \n",
    "          # Compute the mean color of the current superpixel and assign it to the output image\n",
    "          mean_intensity = cv2.mean(input, mask=class_mask)[0]\n",
    "          superpixelMeansList[i][class_mask > 0] = mean_intensity\n",
    "\n",
    "    return superpixelMeansList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kmeans_Segmentation(list_imgs, k, num_scales):\n",
    "    '''\n",
    "    Performs k-means segmentation on all the images in list_imgs\n",
    "\n",
    "    Args:\n",
    "        list_imgs (list): list of images in which K-means segmentation will be applied\n",
    "        k (int): number of clusters for K-means segmentation\n",
    "        num_scales (int): number of scales\n",
    "\n",
    "    Output:\n",
    "        KMeansMaskList (list): 1D vector of size equal to num_scales that contains the binary images resulting of the K-means segmentation\n",
    "    '''\n",
    "    KMeansMaskList  = [None] * num_scales\n",
    "\n",
    "    for i in range(len(list_imgs)):\n",
    "        # Flatten the superpixel means into a 2D array\n",
    "        imgClustered_flat = list_imgs[i].reshape((-1, 1))\n",
    "\n",
    "        # Perform k-means clustering with k clusters\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0, n_init=10).fit(imgClustered_flat)\n",
    "\n",
    "        # Reshape the cluster labels back into the original image shape\n",
    "        cluster_labels = kmeans.labels_.reshape(list_imgs[i].shape)\n",
    "\n",
    "        centroids = kmeans.cluster_centers_\n",
    "\n",
    "        # Obtaining the binary mask that corresponds to the cluster with highest intensity\n",
    "        KMeansMaskList[i] = np.zeros_like(list_imgs[i])\n",
    "        KMeansMaskList[i][cluster_labels== np.argmax(centroids[:,0])] = 255\n",
    "\n",
    "    return KMeansMaskList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GT_Mask(cropped_gt):\n",
    "    ''' \n",
    "    Finds the contours of the ground truth and creates a binary image per each mass\n",
    "\n",
    "    Arg:\n",
    "      cropped_gt (numpy.ndarray): cropped ground truth image\n",
    "\n",
    "    Output:\n",
    "      vector_gt_mask (list): list that contains one binary image per mass\n",
    "      contours_gt (list): contours of the ground truth\n",
    "    '''\n",
    "\n",
    "    vector_gt_mask = []\n",
    "\n",
    "    contours_gt, _ = cv2.findContours(cropped_gt, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    # Create a binary mask per mass\n",
    "    for i in range(len(contours_gt)):\n",
    "      mask = np.zeros_like(cropped_gt)\n",
    "      cv2.drawContours(mask, contours_gt, i, (255, 255, 255), cv2.FILLED)\n",
    "      vector_gt_mask.append(mask)\n",
    "\n",
    "    return vector_gt_mask,contours_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU_calculation(vector_gt_mask,contours_vector,contours_gt):\n",
    "  \"\"\"\n",
    "  Calculate the Intersection Over Union (IOU) between two binary images.\n",
    "\n",
    "  Args:\n",
    "      vector_gt_mask (list): list containing one binary mask per mass of the ground truth\n",
    "      contours_vector (list): contous of the candidate regions\n",
    "      contours_gt (list): contours of the ground truth\n",
    "\n",
    "  Output:\n",
    "      IOU_vector (list): list that contains the maximum IOU for each mass\n",
    "  \"\"\"\n",
    "\n",
    "  IOU_vector = []\n",
    "\n",
    "  for k in range(len(contours_gt)):\n",
    "    max_iou = 0\n",
    "    for contours in contours_vector:\n",
    "      for contour in contours:\n",
    "        mask_candidate = np.zeros_like(vector_gt_mask[k])\n",
    "        cv2.drawContours(mask_candidate, [contour], 0, (255, 255, 255), cv2.FILLED)\n",
    "\n",
    "        # Compute the intersection and union images\n",
    "        intersection = cv2.bitwise_and(mask_candidate, vector_gt_mask[k])\n",
    "        union = cv2.bitwise_or(mask_candidate, vector_gt_mask[k])\n",
    "\n",
    "        # Compute the number of white pixels in the intersection and union images\n",
    "        intersection_pixels = cv2.countNonZero(intersection)\n",
    "        union_pixels = cv2.countNonZero(union)\n",
    "\n",
    "        # Compute the IoU\n",
    "        iou = intersection_pixels / union_pixels\n",
    "\n",
    "        if iou > max_iou:\n",
    "          max_iou = iou\n",
    "\n",
    "    IOU_vector.append(max_iou)\n",
    "\n",
    "    return IOU_vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Main code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image number:  0\n",
      "Processing image number:  1\n",
      "Processing image number:  2\n",
      "Processing image number:  3\n",
      "Processing image number:  4\n",
      "Processing image number:  5\n",
      "Processing image number:  6\n",
      "Processing image number:  7\n",
      "Processing image number:  8\n",
      "Processing image number:  9\n",
      "Processing image number:  10\n",
      "Processing image number:  11\n",
      "Processing image number:  12\n",
      "Processing image number:  13\n",
      "Processing image number:  14\n",
      "Processing image number:  15\n",
      "Processing image number:  16\n",
      "Processing image number:  17\n",
      "Processing image number:  18\n",
      "Processing image number:  19\n",
      "Processing image number:  20\n",
      "Processing image number:  21\n",
      "Processing image number:  22\n",
      "Processing image number:  23\n",
      "Processing image number:  24\n",
      "Processing image number:  25\n",
      "Processing image number:  26\n",
      "Processing image number:  27\n",
      "Processing image number:  28\n",
      "Processing image number:  29\n",
      "Processing image number:  30\n",
      "Processing image number:  31\n",
      "Processing image number:  32\n",
      "Processing image number:  33\n",
      "Processing image number:  34\n",
      "Processing image number:  35\n",
      "Processing image number:  36\n",
      "Processing image number:  37\n",
      "Processing image number:  38\n",
      "Processing image number:  39\n",
      "Processing image number:  40\n",
      "Processing image number:  41\n",
      "Processing image number:  42\n",
      "Processing image number:  43\n",
      "Processing image number:  44\n",
      "Processing image number:  45\n",
      "Processing image number:  46\n",
      "Processing image number:  47\n",
      "Processing image number:  48\n",
      "Processing image number:  49\n",
      "Processing image number:  50\n",
      "Processing image number:  51\n",
      "Processing image number:  52\n",
      "Processing image number:  53\n",
      "Processing image number:  54\n",
      "Processing image number:  55\n",
      "Processing image number:  56\n",
      "Processing image number:  57\n",
      "Processing image number:  58\n",
      "Processing image number:  59\n",
      "Processing image number:  60\n",
      "Processing image number:  61\n",
      "Processing image number:  62\n",
      "Processing image number:  63\n",
      "Processing image number:  64\n",
      "Processing image number:  65\n",
      "Processing image number:  66\n",
      "Processing image number:  67\n",
      "Processing image number:  68\n",
      "Processing image number:  69\n",
      "Processing image number:  70\n",
      "Processing image number:  71\n",
      "Processing image number:  72\n",
      "Processing image number:  73\n",
      "Processing image number:  74\n",
      "Processing image number:  75\n",
      "Processing image number:  76\n",
      "Processing image number:  77\n",
      "Processing image number:  78\n",
      "Processing image number:  79\n",
      "Processing image number:  80\n",
      "Processing image number:  81\n",
      "Processing image number:  82\n",
      "Processing image number:  83\n",
      "Processing image number:  84\n",
      "Processing image number:  85\n",
      "Processing image number:  86\n",
      "Processing image number:  87\n",
      "Processing image number:  88\n",
      "Processing image number:  89\n",
      "Processing image number:  90\n",
      "Processing image number:  91\n",
      "Processing image number:  92\n",
      "Processing image number:  93\n",
      "Processing image number:  94\n",
      "Processing image number:  95\n",
      "Processing image number:  96\n",
      "Processing image number:  97\n",
      "Processing image number:  98\n",
      "Processing image number:  99\n",
      "Processing image number:  100\n",
      "Processing image number:  101\n",
      "Processing image number:  102\n",
      "Processing image number:  103\n",
      "Processing image number:  104\n",
      "Processing image number:  105\n",
      "Processing image number:  106\n",
      "Processing image number:  107\n",
      "Processing image number:  108\n",
      "Processing image number:  109\n",
      "Processing image number:  110\n",
      "Processing image number:  111\n",
      "Processing image number:  112\n",
      "Processing image number:  113\n",
      "Processing image number:  114\n",
      "Processing image number:  115\n",
      "Processing image number:  116\n",
      "Processing image number:  117\n",
      "Processing image number:  118\n",
      "Processing image number:  119\n",
      "Processing image number:  120\n",
      "Processing image number:  121\n",
      "Processing image number:  122\n",
      "Processing image number:  123\n",
      "Processing image number:  124\n",
      "Processing image number:  125\n",
      "Processing image number:  126\n",
      "Processing image number:  127\n",
      "Processing image number:  128\n",
      "Processing image number:  129\n",
      "Processing image number:  130\n",
      "Processing image number:  131\n",
      "Processing image number:  132\n",
      "Processing image number:  133\n",
      "Processing image number:  134\n",
      "Processing image number:  135\n",
      "Processing image number:  136\n",
      "Processing image number:  137\n",
      "Processing image number:  138\n",
      "Processing image number:  139\n",
      "Processing image number:  140\n",
      "Processing image number:  141\n",
      "Processing image number:  142\n",
      "Processing image number:  143\n",
      "Processing image number:  144\n",
      "Processing image number:  145\n",
      "Processing image number:  146\n",
      "Processing image number:  147\n",
      "Processing image number:  148\n",
      "Processing image number:  149\n",
      "Processing image number:  150\n",
      "Processing image number:  151\n",
      "Processing image number:  152\n",
      "Processing image number:  153\n",
      "Processing image number:  154\n",
      "Processing image number:  155\n",
      "Processing image number:  156\n",
      "Processing image number:  157\n",
      "Processing image number:  158\n",
      "Processing image number:  159\n",
      "Processing image number:  160\n",
      "Processing image number:  161\n",
      "Processing image number:  162\n",
      "Processing image number:  163\n",
      "Processing image number:  164\n",
      "Processing image number:  165\n",
      "Processing image number:  166\n",
      "Processing image number:  167\n",
      "Processing image number:  168\n",
      "Processing image number:  169\n",
      "Processing image number:  170\n",
      "Processing image number:  171\n",
      "Processing image number:  172\n",
      "Processing image number:  173\n",
      "Processing image number:  174\n",
      "Processing image number:  175\n",
      "Processing image number:  176\n",
      "Processing image number:  177\n",
      "Processing image number:  178\n",
      "Processing image number:  179\n",
      "Processing image number:  180\n",
      "Processing image number:  181\n",
      "Processing image number:  182\n",
      "Processing image number:  183\n",
      "Processing image number:  184\n",
      "Processing image number:  185\n",
      "Processing image number:  186\n",
      "Processing image number:  187\n",
      "Processing image number:  188\n",
      "Processing image number:  189\n",
      "Processing image number:  190\n",
      "Processing image number:  191\n",
      "Processing image number:  192\n",
      "Processing image number:  193\n",
      "Processing image number:  194\n",
      "Processing image number:  195\n",
      "Processing image number:  196\n",
      "Processing image number:  197\n",
      "Processing image number:  198\n",
      "Processing image number:  199\n",
      "Processing image number:  200\n",
      "Processing image number:  201\n",
      "Processing image number:  202\n",
      "Processing image number:  203\n",
      "Processing image number:  204\n",
      "Processing image number:  205\n",
      "Processing image number:  206\n",
      "Processing image number:  207\n",
      "Processing image number:  208\n",
      "Processing image number:  209\n",
      "Processing image number:  210\n",
      "Processing image number:  211\n",
      "Processing image number:  212\n",
      "Processing image number:  213\n",
      "Processing image number:  214\n",
      "Processing image number:  215\n",
      "Processing image number:  216\n",
      "Processing image number:  217\n",
      "Processing image number:  218\n",
      "Processing image number:  219\n",
      "Processing image number:  220\n",
      "Processing image number:  221\n",
      "Processing image number:  222\n",
      "Processing image number:  223\n",
      "Processing image number:  224\n",
      "Processing image number:  225\n",
      "Processing image number:  226\n",
      "Processing image number:  227\n",
      "Processing image number:  228\n",
      "Processing image number:  229\n",
      "Processing image number:  230\n",
      "Processing image number:  231\n",
      "Processing image number:  232\n",
      "Processing image number:  233\n",
      "Processing image number:  234\n",
      "Processing image number:  235\n",
      "Processing image number:  236\n",
      "Processing image number:  237\n",
      "Processing image number:  238\n",
      "Processing image number:  239\n",
      "Processing image number:  240\n",
      "Processing image number:  241\n",
      "Processing image number:  242\n",
      "Processing image number:  243\n",
      "Processing image number:  244\n",
      "Processing image number:  245\n",
      "Processing image number:  246\n",
      "Processing image number:  247\n",
      "Processing image number:  248\n",
      "Processing image number:  249\n",
      "Processing image number:  250\n",
      "Processing image number:  251\n",
      "Processing image number:  252\n",
      "Processing image number:  253\n",
      "Processing image number:  254\n",
      "Processing image number:  255\n",
      "Processing image number:  256\n",
      "Processing image number:  257\n",
      "Processing image number:  258\n",
      "Processing image number:  259\n",
      "Processing image number:  260\n",
      "Processing image number:  261\n",
      "Processing image number:  262\n",
      "Processing image number:  263\n",
      "Processing image number:  264\n",
      "Processing image number:  265\n",
      "Processing image number:  266\n",
      "Processing image number:  267\n",
      "Processing image number:  268\n",
      "Processing image number:  269\n",
      "Processing image number:  270\n",
      "Processing image number:  271\n",
      "Processing image number:  272\n",
      "Processing image number:  273\n",
      "Processing image number:  274\n",
      "Processing image number:  275\n",
      "Processing image number:  276\n",
      "Processing image number:  277\n",
      "Processing image number:  278\n",
      "Processing image number:  279\n",
      "Processing image number:  280\n",
      "Processing image number:  281\n",
      "Processing image number:  282\n",
      "Processing image number:  283\n",
      "Processing image number:  284\n",
      "Processing image number:  285\n",
      "Processing image number:  286\n",
      "Processing image number:  287\n",
      "Processing image number:  288\n",
      "Processing image number:  289\n",
      "Processing image number:  290\n",
      "Processing image number:  291\n",
      "Processing image number:  292\n",
      "Processing image number:  293\n",
      "Processing image number:  294\n",
      "Processing image number:  295\n",
      "Processing image number:  296\n",
      "Processing image number:  297\n",
      "Processing image number:  298\n",
      "Processing image number:  299\n",
      "Processing image number:  300\n",
      "Processing image number:  301\n",
      "Processing image number:  302\n",
      "Processing image number:  303\n",
      "Processing image number:  304\n",
      "Processing image number:  305\n",
      "Processing image number:  306\n",
      "Processing image number:  307\n",
      "Processing image number:  308\n",
      "Processing image number:  309\n",
      "Processing image number:  310\n",
      "Processing image number:  311\n",
      "Processing image number:  312\n",
      "Processing image number:  313\n",
      "Processing image number:  314\n",
      "Processing image number:  315\n",
      "Processing image number:  316\n",
      "Processing image number:  317\n",
      "Processing image number:  318\n",
      "Processing image number:  319\n",
      "Processing image number:  320\n",
      "Processing image number:  321\n",
      "Processing image number:  322\n",
      "Processing image number:  323\n",
      "Processing image number:  324\n",
      "Processing image number:  325\n",
      "Processing image number:  326\n",
      "Processing image number:  327\n",
      "Processing image number:  328\n",
      "Processing image number:  329\n",
      "Processing image number:  330\n",
      "Processing image number:  331\n",
      "Processing image number:  332\n",
      "Processing image number:  333\n",
      "Processing image number:  334\n",
      "Processing image number:  335\n",
      "Processing image number:  336\n",
      "Processing image number:  337\n",
      "Processing image number:  338\n",
      "Processing image number:  339\n",
      "Processing image number:  340\n",
      "Processing image number:  341\n",
      "Processing image number:  342\n",
      "Processing image number:  343\n",
      "Processing image number:  344\n",
      "Processing image number:  345\n",
      "Processing image number:  346\n",
      "Processing image number:  347\n",
      "Processing image number:  348\n",
      "Processing image number:  349\n",
      "Processing image number:  350\n",
      "Processing image number:  351\n",
      "Processing image number:  352\n",
      "Processing image number:  353\n",
      "Processing image number:  354\n",
      "Processing image number:  355\n",
      "Processing image number:  356\n",
      "Processing image number:  357\n",
      "Processing image number:  358\n",
      "Processing image number:  359\n",
      "Processing image number:  360\n",
      "Processing image number:  361\n",
      "Processing image number:  362\n",
      "Processing image number:  363\n",
      "Processing image number:  364\n",
      "Processing image number:  365\n",
      "Processing image number:  366\n",
      "Processing image number:  367\n",
      "Processing image number:  368\n",
      "Processing image number:  369\n",
      "Processing image number:  370\n",
      "Processing image number:  371\n",
      "Processing image number:  372\n",
      "Processing image number:  373\n",
      "Processing image number:  374\n",
      "Processing image number:  375\n",
      "Processing image number:  376\n",
      "Processing image number:  377\n",
      "Processing image number:  378\n",
      "Processing image number:  379\n",
      "Processing image number:  380\n",
      "Processing image number:  381\n",
      "Processing image number:  382\n",
      "Processing image number:  383\n",
      "Processing image number:  384\n",
      "Processing image number:  385\n",
      "Processing image number:  386\n",
      "Processing image number:  387\n",
      "Processing image number:  388\n",
      "Processing image number:  389\n",
      "Processing image number:  390\n",
      "Processing image number:  391\n",
      "Processing image number:  392\n",
      "Processing image number:  393\n",
      "Processing image number:  394\n",
      "Processing image number:  395\n",
      "Processing image number:  396\n",
      "Processing image number:  397\n",
      "Processing image number:  398\n",
      "Processing image number:  399\n",
      "Processing image number:  400\n",
      "Processing image number:  401\n",
      "Processing image number:  402\n",
      "Processing image number:  403\n",
      "Processing image number:  404\n",
      "Processing image number:  405\n",
      "Processing image number:  406\n",
      "Processing image number:  407\n",
      "Processing image number:  408\n",
      "Processing image number:  409\n"
     ]
    }
   ],
   "source": [
    "# Sort the files in the directory in which we will iterate:\n",
    "dir_images_sorted = sorted(os.listdir(path_images))\n",
    "\n",
    "# Define an empty vector in which we will store the IOU per mass in the positive images\n",
    "IOU_vector_final=[]\n",
    "\n",
    "# Number of scales that will be used in the multi-scale grayscale morphology\n",
    "num_scales = 11\n",
    "\n",
    "# Sizes of structuring elements that will be used for multi-scale grayscale morphology\n",
    "size_opening = [8, 16, 24, 32, 40, 48, 56, 64, 72, 92, 120]\n",
    "size_topHat = [16, 32, 48, 64, 80, 96, 112, 128, 144, 184, 240]\n",
    "\n",
    "# Maximum and minimum areas for filtering the contours in the segmented image\n",
    "min_area = 2000\n",
    "max_area = 800000\n",
    "\n",
    "# Main loop in which we iterate for all the images (positive and negative):\n",
    "for index, name_image in enumerate(dir_images_sorted): \n",
    "  # To override undesired files:\n",
    "  if name_image!=\"Thumbs.db\" and name_image!=\".DS_Store\":\n",
    "    print('Processing image number: ', index)\n",
    "    img_breast = cv2.imread(path_images +  name_image, cv2.IMREAD_GRAYSCALE)\n",
    "    # Create a boolean dummy variable to check if image is positive or negative:\n",
    "    if not os.path.exists(os.path.join(path_gt, name_image)):\n",
    "        is_positive = False\n",
    "    else:\n",
    "        is_positive = True\n",
    "        img_gt = cv2.imread(path_gt +  name_image, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    ##################### PREPROCESSING #####################\n",
    "    # Downsampling the breast image in a factor of 4:\n",
    "    downsampled_img = cv2.resize(img_breast, (img_breast.shape[1] // 4, img_breast.shape[0] // 4), interpolation=cv2.INTER_CUBIC)\n",
    "    # Cropping the image to keep only the breast (remove part of the background):\n",
    "    cropped_img,cropped_gt=function_crop(downsampled_img, is_positive, img_gt)\n",
    "    # Apply CLAHE to the cropped downsampled image:\n",
    "    CLAHE_img = Clahe(cropped_img, clip_limit=10.0, tile_size=6)\n",
    "    # Mean shift filtering\n",
    "    meanShiftImage = MeanShift(CLAHE_img, sp=5, sr=5)\n",
    "\n",
    "    ##################### MULTI-SCALE GRAYSCALE MORPHOLOGY #####################\n",
    "    # Apply grayscale morphology for different structuring element sizes (multiscale)\n",
    "    morphologyList=MultiScaleMorphology(meanShiftImage, num_scales, size_opening, size_topHat)\n",
    "\n",
    "    ##################### SEGMENTATION #####################\n",
    "    # Apply Gaussian blurring to all the images in morphologyList\n",
    "    imgsBlurredList = GaussianBlurring(morphologyList, kernel_size=3)\n",
    "    # Superpixel generation\n",
    "    superpixelsMeanList = Superpixel(imgsBlurredList, num_scales=num_scales, size_opening=size_opening, num_iterations=10)\n",
    "    # K-means segmentation \n",
    "    KMeansMaskList = Kmeans_Segmentation(superpixelsMeanList, k=4, num_scales=num_scales)\n",
    "\n",
    "\n",
    "    # Upsampling of the cropped original image\n",
    "    originalUpsampled = cv2.resize(cropped_img, None, fx=4, fy=4, interpolation=cv2.INTER_NEAREST)\n",
    "    cv2.imwrite(f'/Users/clara/Desktop/MAIA/AIA/cropped_imgs_upsampled_all/{name_image}', originalUpsampled)\n",
    "\n",
    "    # Upsample and find contours in the segmented images of all the scales\n",
    "    segmentedUpsampledList = [None]*num_scales\n",
    "    contours_vector = []  # vector that contains the contours of all the candidates of all the scales of one image\n",
    "    for i in range(num_scales):\n",
    "        segmentedUpsampledImage = cv2.resize(KMeansMaskList[i], None, fx=4, fy=4, interpolation=cv2.INTER_NEAREST)\n",
    "        contours, _ = cv2.findContours(segmentedUpsampledImage, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        # Filter the contours by area\n",
    "        filtered_contours = []\n",
    "        for contour in contours:\n",
    "            contour_area = cv2.contourArea(contour)\n",
    "            if min_area <= contour_area <= max_area:\n",
    "                filtered_contours.append(contour)\n",
    "        contours_vector.append(filtered_contours)\n",
    "\n",
    "        # Create a binary image with the segmentation of the scale i\n",
    "        final_segmented_image = np.zeros_like(segmentedUpsampledImage)\n",
    "        cv2.drawContours(final_segmented_image, filtered_contours, -1, (255, 255, 255), cv2.FILLED)\n",
    "        cv2.imwrite(f'/Users/clara/Desktop/MAIA/AIA/segmented_images_new/{name_image[:-4]}_{i}.tif', final_segmented_image)\n",
    "\n",
    "    if is_positive:\n",
    "        cv2.imwrite(f'/Users/clara/Desktop/MAIA/AIA/cropped_gt_all/{name_image}', cropped_gt)\n",
    "        # Obtain ground truth mask and contours\n",
    "        vector_gt_mask,contours_gt=GT_Mask(cropped_gt)\n",
    "        # Compute the IOU of the regions obtained with the segmentation with the ground truth regions\n",
    "        IOU_vector=IOU_calculation(vector_gt_mask,contours_vector,contours_gt)\n",
    "        IOU_vector_final.append(IOU_vector)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3226558748885383], [0.5072491846685395], [0.4045369794903667], [0.6570442286947141], [0.8294031799677395], [0.8507967998619886], [0.8284321272773929], [0.4516714174656647], [0.76287852013797], [0.8659996534162567], [0.5066008587135348], [0.3112340378687803], [0.824371520453681], [0.7096774193548387], [0.7366737114007279], [0.8240536005955622], [0.7586821844472653], [0.8769987746111457], [0.8561111100005797], [0.8270817165358857], [0.7673592799634357], [0.8314774234903031], [0.8907261831888914], [0.9044448959590768], [0.8701410987691384], [0.8015566550018101], [0.7961749940234282], [0.7797289642250207], [0.7409301486071836], [0.6414518565704128], [0.8415248157154558], [0.4657362428144777], [0.665372460496614], [0.6405810426018126], [0.8275173053593092], [0.8411445611680097], [0.7065168560579193], [0.6605054634328112], [0.7126762012552732], [0.8422300660693275], [0.7413704489687645], [0.7641313660161828], [0.6083427821209046], [0.9129595616847741], [0.8081130065660821], [0.8218060559144907], [0.8415756181713628], [0.7824126945963815], [0.7172724840423499], [0.8849046914176962], [0.6773623922488905], [0.7116992698518143], [0.5345831854527234], [0.8544763070466865], [0.841293674627008], [0.8461360695137733], [0.837955840159674], [0.6391193275464234], [0.4053274432558063], [0.971597069920566], [0.530345622585799], [0.814438923897237], [0.8178893913241944], [0.7543883337834189], [0.29282098481362173], [0.6640055536272128], [0.6172326028927928], [0.7695383194445022], [0.5482390699729087], [0.8230537834313612], [0.8825530010137226], [0.6059660394676457], [0.72577846331823], [0.6943285072391413], [0.7552220977976857], [0.7224007643533397], [0.7795350009451201], [0.7116046961812103], [0.7583687006380168], [0.7347183061126775], [0.6806564984474699], [0], [0.45665171898355755], [0.6149977137631458], [0.8742618826956177], [0.917815407851118], [0.8402410488187967], [0.8447370514758533], [0.8055670025849474], [0.7429427200678559], [0.8326586132072417], [0.7772054544596656], [0.7852899426949161], [0.777352959918435], [0.6069305289087852], [0.8245536698258761], [0.8325390980375168], [0.6807769304827544], [0.3885161415791521], [0.7745434723357864], [0.9012363630405889], [0.624177225314407], [0.8019065220563202], [0.8807627323195751], [0.7265837699840217], [0.714938037683445], [0.8143720617864338]]\n",
      "0.9906542056074766\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sensitivity of the method\n",
    "tp = 0\n",
    "fn = 0\n",
    "print(IOU_vector_final)\n",
    "for i in range(len(IOU_vector_final)):\n",
    "  if IOU_vector_final[i][0] > 0.2:\n",
    "    tp = tp+1\n",
    "  else:\n",
    "    fn = fn+1\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(sensitivity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
